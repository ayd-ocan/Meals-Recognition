{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ocanaydin/meals-recognition-tfl-basic?scriptVersionId=113933745\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Access your google drive.","metadata":{"id":"G3h0Ug0nuF3U"}},{"cell_type":"markdown","source":"## IMPORTANT!!!","metadata":{"id":"WI2q-0tt3nU0"}},{"cell_type":"markdown","source":"I highly recommend that you should use this notebook in google colab.This is a basic implemantation of portuguese meals recognition problem.I tried to explain every part of model.If you have any comments or suggestions about this notebook please write it.Enjoy!!!","metadata":{"id":"upkVbXms3ulo"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount(\"/content/drive\")","metadata":{"id":"qb7C7Him1ECw","outputId":"ebe0658f-e18b-4605-a963-d294a8cf2dbd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 1** : Send images to google drive as their categories.","metadata":{"id":"WWx7qO5KuQTm"}},{"cell_type":"markdown","source":"### (1)Create train and validation directories in drive.","metadata":{"id":"9Lgf-Swsualt"}},{"cell_type":"code","source":"import os\n\"\"\"This is your base directory path.Indeed,it indicates where you want to put all the images as their directories.\"\"\"\n\"\"\"Below is an example path which i use for this project.\"\"\"\nbase_dir = \"/content/drive/My Drive/Portuguese_Meals_Recognition_Test\"\ndirectories = [\"train\",\"validation\"]\n\"\"\"We have two directories in this project as shown above.We can create new directories as train and validation in base dir.\"\"\"\nfor i in range(len(directories)):\n  folder = os.path.join(base_dir,directories[i])\n  os.mkdir(folder)","metadata":{"id":"cIHMExKfuLl7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###(2)Extract zip file to your base directory.","metadata":{"id":"9ZtROSx5vqBY"}},{"cell_type":"code","source":"import zipfile as zf\n\"\"\"First download zip file your pc.After that you can upload zip file to your base directory using drive.\"\"\"\n\"\"\"I have changed the name of zip file.You can use name whatever you want.\"\"\"\nfiles = zf.ZipFile(os.path.join(base_dir,\"meals.zip\"),\"r\")\nfiles.extractall(base_dir)\nfiles.close()\n\"\"\"Now you can see all images in your drive with their directories.\"\"\"","metadata":{"id":"E93rBsIzv5ZE","outputId":"0ec3086e-25d0-41b1-a2bf-a0a8d10542fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###(3)Create subdirectories with class names in train and validation directories.","metadata":{"id":"2yRKdhejxJSQ"}},{"cell_type":"code","source":"\"\"\"We define images path which we extracted in second step.You can see portuguese_food_2 folder in your drive\"\"\"\nmain_images_path = os.path.join(base_dir,\"portuguese_food_2\")\n\"\"\"We can get the class names from their directory names.\"\"\"\nclasses = os.listdir(main_images_path)","metadata":{"id":"xzAZz611xj1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Here,we create a subdirectory in both train and validation directories for every class name.\"\"\"\nfor i in range(len(directories)):\n  for j in range(len(classes)):\n    folder = os.path.join(base_dir,directories[i])\n    folder = os.path.join(folder,classes[j])\n    os.mkdir(folder)","metadata":{"id":"NrAl-Nr3yoau"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###(4)Copy all images to train and validation directories as their belonged class. (Train has %80 ,Validation %20)","metadata":{"id":"VSuhySMqzT61"}},{"cell_type":"code","source":"import shutil\n\"\"\"Here we see 3 path.First is assigned above but we can assign it again.Train path and validation path are our directories which we want to copy images to them.\"\"\"\nmain_images_path = os.path.join(base_dir,\"portuguese_food_2\")\ntrain_path = os.path.join(base_dir,\"train\")\nvalidation_path = os.path.join(base_dir,\"validation\")\n\"\"\"Here i have advices for you.First use train_path in below loop and define range in 0 to int(n * 0.8) as given. (Train images have to be %80 of all images.)\"\"\"\n\"\"\"Secondly,you can change train_path to validation_path.And you should also change to range in int(n * 0.8 ) to int(n) (Validation images have to be %20 of all images.)\"\"\"\nfor filename in os.listdir(main_images_path):\n  images_path = os.path.join(main_images_path,filename)\n  n = len(os.listdir(images_path))\n  for i in range(int(n * 0.8),int(n)):\n    src = os.path.join(images_path,os.listdir(images_path)[i])\n    dst = os.path.join(validation_path,filename)\n    shutil.copy(src,dst)","metadata":{"id":"y6fmyMzLz6aA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###(5)Ensure that images whether any of them are corrupted.","metadata":{"id":"62uU4PQo2DLD"}},{"cell_type":"code","source":"from os import listdir\nfrom PIL import Image\ndef check_corrupted(directory_type,class_name):\n  path = base_dir+\"/{}/{}/\".format(directory_type,class_name)\n  for filename in listdir(path):\n      if filename.endswith('.jpg'):\n          try:\n              img = Image.open(path+filename)  # open the image file\n              img.verify()  # verify that it is, in fact an image\n          except (IOError, SyntaxError) as e:\n              print(filename)\n              os.remove(path+filename)","metadata":{"id":"RtJE004c2cAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Use checking function in below for both train and validation directories.If you see corrupted image,you can delete it.\"\"\"\nfor i in range(len(directories)):\n  for j in range(len(classes)):\n    check_corrupted(directories[i],classes[j])","metadata":{"id":"47W8kC0r2lNF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (6)See the numbers of images in directories.","metadata":{"id":"Qw7Chfa03GR6"}},{"cell_type":"code","source":"for i in range(len(classes)):\n  print(classes[i])\n  print(len(os.listdir(os.path.join(train_path,classes[i]))))\n  print(len(os.listdir(os.path.join(validation_path,classes[i]))))","metadata":{"id":"hKYNMWYX3MKI","outputId":"68203ac8-50ef-489d-b2ec-c2ac81b827e9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Step 2** : Image Data Generator for training and validation.","metadata":{"id":"4k1oWJgY1WrB"}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"id":"zO1a-0cL1Zlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###(1)Training","metadata":{"id":"Kw3LMhQfeeZQ"}},{"cell_type":"code","source":"import os\n\"\"\"You dont need to reassing these two pth but i done it for convenience.\"\"\"\nbase_dir = \"/content/drive/My Drive/Portuguese_Meals_Recognition_Test\"\ntrain_path = os.path.join(base_dir,\"train\")\n\"\"\"All images will be scaled by 1/255 to normalize.(pixels between 0-1).Also image augmentation is used.\"\"\"\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1./255,\n    rotation_range = 40,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = \"nearest\"\n)\n\"\"\"We can create our train generator.\"\"\"\ntrain_generator = train_datagen.flow_from_directory(train_path,target_size = (300,300),class_mode = \"categorical\",batch_size = 128)","metadata":{"id":"ZWFOkXLO1e1O","outputId":"639664ba-4691-408a-94bb-dffb17f3040c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_path = os.path.join(base_dir,\"validation\")\n\"\"\"All images will be scaled by 1/255 to normalize.(pixels between 0-1).Also image augmentation is NOT used for validation dataset.\"\"\"\nvalidation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1./255\n)\n\"\"\"We can create our validation generator.\"\"\"\nvalidation_generator = validation_datagen.flow_from_directory(validation_path,target_size = (300,300),class_mode = \"categorical\",batch_size = 128)","metadata":{"id":"u4RmP-O91xKd","outputId":"ddeae713-c0a2-41c5-be02-e55e6f07545e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Step 3** : CNN Archictecture with Transfer Learning","metadata":{"id":"jGmaB6dH2SYD"}},{"cell_type":"markdown","source":"###(1)Use InceptionV3 as base model.","metadata":{"id":"E25mkJQo2ZB8"}},{"cell_type":"code","source":"\"\"\"Here i used InceptionV3 for base model.You can use other models if you want.\"\"\"\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nbase_model = InceptionV3(input_shape = (300,300,3),include_top = False,weights = \"imagenet\")\n\"\"\"Freeze all layers to stop updating InceptionV3 trained weights. \"\"\"\nfor layer in base_model.layers:\n  layer.trainable = False","metadata":{"id":"GyVg1TqE2MPl","outputId":"3d3d805d-5cc3-4c52-cb1e-f5220e255230"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###(2)Create an architecture to feed model with images.","metadata":{"id":"RjUlXflF3JFH"}},{"cell_type":"code","source":"import tensorflow \nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras import layers \n","metadata":{"id":"7UH_0-is3CUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Here we can get the last layer as mixed8.It means that we can start updating our weights from mixed8 layer.You can change it if you want.\"\"\"\nlast_layer = base_model.get_layer(\"mixed8\")\nprint(last_layer.output_shape)\n\"\"\"Flatten layer to reduce output shape to 1 dim.\"\"\"\nx = layers.Flatten()(last_layer.output)\n\"\"\"Add fully connected layers to 512 units.\"\"\"\nx = layers.Dense(512,activation = \"relu\")(x)\n\"\"\"Add dropout layer.\"\"\"\nx = layers.Dropout(0.2)(x)\n\"\"\"Output Layer which contains number of classes units (23)\"\"\"\nclasses = os.listdir(train_path)\nx = layers.Dense(len(classes),activation = \"softmax\")(x)\n\"\"\"Here we can connect our model end to end.\"\"\"\nmodel = tensorflow.keras.models.Model(base_model.input,x)","metadata":{"id":"jetQ34_C3feo","outputId":"be2c10aa-1bff-4150-a3bf-9a110ec163e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"RVianHpI3xdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Step 4** :Compile and fit the model","metadata":{"id":"gfY5bekj4x3a"}},{"cell_type":"code","source":"\"\"\"I used RMSprop optimizer here.You can change if you want.\"\"\"\nmodel.compile(RMSprop(learning_rate = 0.001),loss = \"categorical_crossentropy\",metrics = [\"acc\"])","metadata":{"id":"D4DCfX614qqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"This is our callback class.Using this callback,we guaranteed if accuracy of model > 0.95,training will be stopped.It prevents overfitting.\"\"\"\nclass MyCallback(tensorflow.keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,logs = {}):\n    if logs.get(\"acc\") >= 0.95:\n      print(\"Model reached accuracy %95.Stop it.\")\n      self.model.stop_training = True\ncallback = MyCallback()\n\"\"\"Here we can start train model.\"\"\"\nhistory = model.fit(train_generator,epochs = 20,batch_size = 128,validation_data = validation_generator,verbose = 1,callbacks = [callback])","metadata":{"id":"daWL5JP9494Y","outputId":"47a7e989-e8eb-475e-a56b-47e83702d4d3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 5** : Plot accuracies and losses of model","metadata":{"id":"3VeStvIfK4wR"}},{"cell_type":"code","source":"\"\"\"After training we reached %91.62 accuracy without overfitting.It is not a bad result.Here we can draw to compare training and validation metrics.\"\"\"\nimport matplotlib.pyplot as plt \nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(len(acc)) ","metadata":{"id":"wUuxwczI57aE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs,acc,\"r\",label = \"Training Accuracy\")\nplt.plot(epochs,val_acc,\"b\",label = \"Validation Accuracy\")\nplt.title(\"Accuracies\")\nplt.legend()\nplt.show()","metadata":{"id":"0f_g9TWgLM1K","outputId":"4e6fd335-eb3a-44ec-921c-da1934c349f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs,loss,\"r\",label = \"Training Loss\")\nplt.plot(epochs,val_loss,\"b\",label = \"Validation Loss\")\nplt.title(\"Losses\")\nplt.legend()\nplt.show()","metadata":{"id":"dc5f5l5ELVSy","outputId":"f05ed77a-97b5-4147-9707-a27bfd697f8a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Step 6** : Save model","metadata":{"id":"Pxv0vPBrLrqm"}},{"cell_type":"code","source":"\"\"\"You should save your model to do not train model again and again.You should assign a part.I change base dir which is assigned above.\"\"\"\nmodel.save(os.path.join(base_dir,\"portuguese_meals_recognition_TFL.h5\"))","metadata":{"id":"nJrDWLCrLfcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{"id":"S4zPEhqvL19R"}},{"cell_type":"code","source":"\"\"\"You can import your saved model to reuse.\"\"\"\nmodel1 = tf.keras.models.load_model(os.path.join(base_dir,\"portuguese_meals_recognition_TFL.h5\"))","metadata":{"id":"ubJEmI-hL0gJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Step 7** : Get the images from internet and process them.","metadata":{"id":"hqU2bghIMCSV"}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\nfrom tensorflow.keras.preprocessing import image  \nimport numpy as np\ndef get_and_process(url):\n  \"\"\"We should send a request to see our chosen image from google.\"\"\"\n  response = requests.get(url)\n  img = Image.open(BytesIO(response.content))\n  \"\"\"img1 is defined to show image using its original shape.\"\"\"\n  img1 = img\n  \"\"\"Resize image for appropriate input shape for model. \"\"\"\n  img = img.resize((300,300))\n  \"\"\"Convert img to numpy array,rescale it,expand dims for model input shape convenince and check vertically.\"\"\"\n  x = image.img_to_array(img)\n  x = x / 255.0\n  x = np.expand_dims(x,axis = 0)\n  img_tensor = np.vstack([x])\n  \"\"\"Return original image and img tensor which we will be using to find prediction of image.\"\"\"\n  return img1,img_tensor ","metadata":{"id":"lSJZrTWLL-mK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#**Final** : Predict Image","metadata":{"id":"hPEEg8FbM3iu"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nclasses = os.listdir(os.path.join(base_dir,\"train\"))\n\"\"\"Here you can select an image and copy its url to here.It must be involved in 23 classes.\"\"\"\nurl = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTtaiSaYU4-6O2BvK7YDdo57BKdzW5ySUTxlQ&usqp=CAU\"\n\"\"\"Use our above function to process img and predict it.\"\"\"\nimg1,test_img = get_and_process(url)\npred = model1.predict(test_img)\n\n\"\"\"Here you can see that pred includes 23 probability numbers.They are here because we used softmax as an optimizer in last layer.We can get index of max probability in pred list.\"\"\"\n\"\"\"This index gives us our predicted class.\"\"\"\nprint(f\"Prediction : {classes[np.argmax(pred)]}\")\n\"\"\"Show image.\"\"\"\nplt.imshow(img1)\nplt.show()\nprint(classes)","metadata":{"id":"MPthIC1pM1SQ","outputId":"dad062da-3678-40bc-eec0-dd1e8f674245"},"execution_count":null,"outputs":[]}]}